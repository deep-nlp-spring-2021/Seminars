{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text Classification with Deep Averaging Network (DAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/678/1*0LezMYWUk3pXptoMdO5M_Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model, which we call a deep averaging network (DAN), is still unordered, but its depth allowsit to capture subtle variations in the input better than the standard NBOW model. Furthermore, computing each layer requires just a single matrix multiplication, so the complexity scales with the number of layers rather than the number of nodes in a parse tree.\n",
    "\n",
    "Paper: **Deep Unordered Composition Rivals Syntactic Methods for Text Classification**\n",
    "\n",
    "Link: https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unordered: Treats as bag of word embeddings\n",
    "* Accuracy can be improved by using a variant of dropout, which randomly drops some of words embeddings before averaging i.e. dropout inspired regularizer\n",
    "* The choice of composition function is not as important as initializing with pre-trained embeddings and using a deep network\n",
    "* Training speed of unordered function and accuracy of syntactic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True, batch_first=True, fix_length=None)\n",
    "LABEL = LabelField(batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.examples[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.examples[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT.build_vocab(trn, vectors=\"charngram.100d\")\n",
    "# TEXT.build_vocab(trn, vectors=\"fasttext.simple.300d\")\n",
    "# TEXT.build_vocab(trn, vectors=\"fasttext.en.300d\")\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.300d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible vectors\n",
    "- charngram.100d\n",
    "- fasttext.en.300d\n",
    "- fasttext.simple.300d\n",
    "- glove.42B.300d\n",
    "- glove.840B.300d\n",
    "- glove.twitter.27B.25d \n",
    "- glove.twitter.27B.50d \n",
    "- glove.twitter.27B.100d \n",
    "- glove.twitter.27B.200d \n",
    "- glove.6B.50d \n",
    "- glove.6B.100d \n",
    "- glove.6B.200d \n",
    "- glove.6B.300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.stoi[\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: is there any problem with the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = BucketIterator.splits(\n",
    "        (train, test),\n",
    "        batch_sizes=(64, 64),\n",
    "        device='cuda',\n",
    "        sort=True,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: take a look BucketIterator class, notice the difference with Iterator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_iter.__iter__()); batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Text Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    def __init__(self, emb_dim, n_layers,\n",
    "                 hidden_size, n_outputs, pad_idx=1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False, padding_idx=1)\n",
    "\n",
    "        modules = []\n",
    "        in_features = emb_dim\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            modules.append(nn.Linear(in_features, hidden_size))\n",
    "            modules.append(nn.ReLU())\n",
    "            in_features = hidden_size\n",
    "\n",
    "        modules.append(nn.Linear(hidden_size, n_outputs))\n",
    "\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "N_LAYERS = 1\n",
    "EMBED_DIM = TEXT.vocab.vectors.shape[1]\n",
    "HIDDEN_SIZE = int(EMBED_DIM*3)\n",
    "N_OUTPUTS = len(LABEL.vocab)\n",
    "EPOCHS = 100\n",
    "model = DAN(EMBED_DIM, N_LAYERS, HIDDEN_SIZE, N_OUTPUTS).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "from torch import autograd\n",
    "\n",
    "def training_loop(model, train_iter, val_iter, loss_func, opt, EPOCHS):\n",
    "    history = {}\n",
    "    history['train_loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['val_acc'] = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        val_acc, train_acc = 0, 0\n",
    "        train_loss, val_loss = 0,0\n",
    "    \n",
    "        model.train() \n",
    "        for batch in train_iter:\n",
    "        \n",
    "            x = batch.text\n",
    "            y = batch.label\n",
    "                \n",
    "            opt.zero_grad()\n",
    "            preds = model(x)\n",
    "            \n",
    "            loss = loss_func(preds, y)\n",
    "                \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (preds.argmax(1) == y).sum().item()\n",
    "        \n",
    "        print(\"Epoch:\", epoch, \"Train loss:\", train_loss/len(train), \"train acc:\", train_acc/len(train))\n",
    "    \n",
    "        model.eval()\n",
    "        for batch in val_iter:\n",
    "        \n",
    "            x = batch.text\n",
    "            y = batch.label\n",
    "        \n",
    "            preds = model(x)\n",
    "            loss = loss_func(preds, y)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "            val_acc += (preds.argmax(1) == y).sum().item()\n",
    "        \n",
    "        print(\"Epoch:\", epoch, \"Val loss:\", val_loss/len(test), \"val acc:\", val_acc/len(test), \"\\n\")\n",
    "    \n",
    "        history['train_loss'].append(train_loss/len(train))\n",
    "        history['val_loss'].append(val_loss/len(test))\n",
    "        history['val_acc'].append(val_acc/len(test))\n",
    "        \n",
    "    return history['train_loss'], history['val_loss'], history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, val_acc = training_loop(model, train_iter, test_iter, loss_func, opt, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "# plt.plot(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://github.com/Pinafore/qb/blob/master/qanta/guesser/dan.py\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "- https://github.com/keitakurita/practical-torchtext/blob/master/Lesson%201%20intro%20to%20torchtext%20with%20text%20classification.ipynb\n",
    "http://www.cse.chalmers.se/~richajo/nlp2019/l2/Text%20classification%20using%20a%20CBoW%20representation.html\n",
    "- https://github.com/miyyer/dan/blob/master/dan_sentiment.py\n",
    "- https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf\n",
    "- https://medium.com/tech-that-works/deep-averaging-network-in-universal-sentence-encoder-465655874a04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@InProceedings{Iyyer:Manjunatha:Boyd-Graber:III}-2015,\n",
    "    Title = {Deep Unordered Composition Rivals Syntactic Methods for Text Classification},\n",
    "    Booktitle = {Association for Computational Linguistics},\n",
    "    Author = {Mohit Iyyer and Varun Manjunatha and Jordan Boyd-Graber and Hal {Daum\\'{e} III}},\n",
    "    Year = {2015},\n",
    "    Location = {Beijing, China}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
