{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text Classification with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True, batch_first=True, fix_length=None)\n",
    "LABEL = LabelField(batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.examples[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.examples[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT.build_vocab(trn, vectors=\"charngram.100d\")\n",
    "# TEXT.build_vocab(trn, vectors=\"fasttext.simple.300d\")\n",
    "# TEXT.build_vocab(trn, vectors=\"fasttext.en.300d\")\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.50d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible vectors\n",
    "- charngram.100d\n",
    "- fasttext.en.300d\n",
    "- fasttext.simple.300d\n",
    "- glove.42B.300d\n",
    "- glove.840B.300d\n",
    "- glove.twitter.27B.25d \n",
    "- glove.twitter.27B.50d \n",
    "- glove.twitter.27B.100d \n",
    "- glove.twitter.27B.200d \n",
    "- glove.6B.50d \n",
    "- glove.6B.100d \n",
    "- glove.6B.200d \n",
    "- glove.6B.300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.stoi[\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: is there any problem with the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. See pool for the bucketing procedure used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = BucketIterator.splits(\n",
    "        (train, test),\n",
    "        batch_sizes=(8, 8),\n",
    "        device='cuda',\n",
    "        sort=True,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_iter.__iter__()); batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Text Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Embedding**\n",
    "\n",
    "Itâ€™s only a lookup table, given the index, it will return the corresponding vector.\n",
    "The vector representation indicated the weighted matrix is initialized as random values and will be updated by backpropagation.\n",
    "\n",
    "**nn.EmbeddingBag**\n",
    "\n",
    "Since nn.EmbeddingBag accumulates the average across the embeddings on the fly, nn.EmbeddingBag can enhance the performance and memory efficiency to process a sequence of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag.from_pretrained(TEXT.vocab.vectors, mode='mean', freeze=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text, None)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Try to replace nn.EmbeddingBag with nn.Embedding in the code, make appropriate changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBED_DIM = TEXT.vocab.vectors.shape[1]\n",
    "NUN_CLASS = len(LABEL.vocab)\n",
    "EPOCHS = 100\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "from torch import autograd\n",
    "\n",
    "def training_loop(model, train_iter, val_iter, loss_func, opt, EPOCHS):\n",
    "    history = {}\n",
    "    history['train_loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['val_acc'] = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        val_acc, train_acc = 0, 0\n",
    "        train_loss, val_loss = 0,0\n",
    "    \n",
    "        model.train() \n",
    "        for batch in train_iter:         \n",
    "        \n",
    "            x = batch.text\n",
    "            y = batch.label\n",
    "                \n",
    "            opt.zero_grad()\n",
    "            preds = model(x)\n",
    "            \n",
    "            loss = loss_func(preds, y)\n",
    "                \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (preds.argmax(1) == y).sum().item()\n",
    "        \n",
    "        print(\"Train loss:\", train_loss/len(train), \" train acc:\", train_acc/len(train))\n",
    "    \n",
    "        model.eval()\n",
    "        for batch in val_iter:\n",
    "        \n",
    "            x = batch.text\n",
    "            y = batch.label\n",
    "        \n",
    "            preds = model(x)\n",
    "            loss = loss_func(preds, y)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "            val_acc += (preds.argmax(1) == y).sum().item()\n",
    "        \n",
    "        print(\"Val loss:\", val_loss/len(test), \" val acc:\", val_acc/len(test), \"\\n\")\n",
    "    \n",
    "        history['train_loss'].append(train_loss/len(train))\n",
    "        history['val_loss'].append(val_loss/len(test))\n",
    "        history['val_acc'].append(val_acc/len(test))\n",
    "        \n",
    "    return history['train_loss'], history['val_loss'], history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, val_acc = training_loop(model, train_iter, test_iter, loss_func, opt, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(train_loss)\n",
    "# plt.plot(val_loss)\n",
    "plt.plot(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "- https://github.com/keitakurita/practical-torchtext/blob/master/Lesson%201%20intro%20to%20torchtext%20with%20text%20classification.ipynb\n",
    "http://www.cse.chalmers.se/~richajo/nlp2019/l2/Text%20classification%20using%20a%20CBoW%20representation.html\n",
    "- https://github.com/miyyer/dan/blob/master/dan_sentiment.py\n",
    "- https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf\n",
    "- https://medium.com/tech-that-works/deep-averaging-network-in-universal-sentence-encoder-465655874a04"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
